---
title: "Predicting Substance Use Disorder Treatment Completion"
author: "Jared Colina, Kara McGaughey & Valerie Sydnor"
date: "05/05/2019"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
    fig_caption: yes
    keep_tex: yes
  html_document:
    code_folding: show
    highlight: pygments 
    toc: yes
    toc_depth: 4
    toc_float: yes
    df_print: kable
theme: paper
geometry: margin=1.8cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, car, ISLR, rpart, corrplot, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, rsample, gbm, SnowballC, ranger, BiocManager, keras, neuralnet, tensorflow, xtable, tidyverse, mapproj, psych, caret, logisticPCA, JOUSBoost, png)
knitr::opts_chunk$set(options(xtable.comment = FALSE))
knitr::opts_chunk$set(comment = " ")
```

# 1. Introduction 

## 1.1. Scope of the Problem

A substance use disorder (SUD) is a medical, brain-based illness characterized by clinically significant impairments in health, social function, and voluntary control over substance use. SUDs range in severity, duration, and complexity. In addition to this inherent heterogeneity in symptomology and symptom severity, SUDs are incredibly wide-spread. In 2015, 20.8 million people aged 12 or older met official diagnostic criteria for a substance use disorder (Medina, 2016). Of these people, only about 1 in 10 received any type of specialty treatment. On top of this, approximately one-third of people who enroll in substance use treatment facilities drop out or are prematurely terminated by the facility (SAMHSA, 2012).

Critically, while treatment completion is associated with higher rates of drug use termination, non-completion is intimately linked with criminal involvement, relapse/readmission, illness chronicity, and even death. **As such, not only is successful treatment completion is a clinical utile outcome measure, but accounting for and acting on treatment completion rates should be a public health priority.**

Successful treatment completion rates can also be used to assess the functionality and efficacy of national and state-level healthcare systems. As such, identifying key predictors of successful treatment (or treatment completion) can serve not only to maximize positive outcomes based on individual patient characteristics, but also to discover disparities in treatment access and -- ultimately -- inform policy to increase treatment success and reduce unmet treatment needs. Specifically, if we are able to a priori identify patients unlikely to complete traditional treatment trajectories, we will be better able to enroll these individuals in evidence-based interventions to diversify treatment opportunities and increase retention rates among this would-be treatment refractory population.

In order to make this leap towards individualized, precision medicine and matching patients to the most effective treatments based on their characteristics, we must first use these characteristics to predict who will succeed at a given treatment. 

## 1.2. Project Goals

As such, the overarching aim of this project is to leverage advanced statistical techniques -- logistic regression, penalized regression, random forest, boosting, and deep learning (i.e., neural networks) -- to identify factors most predictive of successful substance use disorder treatment outcomes. 

To accomplish this, we make use of a large, publicly-available  dataset commonly used in the SUD field to evaluate which method works best at predicting successful treatment using real life patient information.

# 2. Methods and Model Results

## 2.1. The Treatment Episode Data Set—Discharges (TEDS-D) Data

```{r Orig Data, eval=FALSE, include=FALSE}
# Read in Data:
drugdata <- read.csv("./TEDSD2006.csv", header=T, na.strings="-9")
```

### Data Overview

The Treatment Episode Data Set—Discharges (TEDS-D) is a public dataset made available by the US Department of Health and Human Services that contains patient-level information for individuals treated for substance use disorders in the United States during the year 2006.  The TEDS-D data includes variables related to patient demographics (e.g., age, sex, race), geographics (e.g. census division), and psychographics (e.g. employment); patient drug use (e.g. primary substance abused, age of first use, frequency of use); SUD treatment program (e.g. rehabilitation center type, length of treatment stay); and SUD risk factors (e.g. comorbid psychiatric diagnoses, veteran status, living situation). 

The binary outcome variable of interest included in the TEDS-D dataset and used as the dependent variable in all statistical models developed here is **Treatment Completed** (yes=1 or no=0). A **completed treatment** indicates that all parts of the treatment plan were completed prior to patient discharge from the treatment program. On the other hand, **treatment non-completion** denotes that the patient left or was discharged prior to treatment completion. The most frequent causes of treatment non-completion included a patient leaving the program early against professional advice, program termination by the treatment facility due to patient non-compliance, or patient transfer to another facility. 

### Data Cleaning: Selection of Predictor Variables and Discharge Entries

The original TEDS-D dataset included 65 variables and 1,048,575 observations. However, each observation included in this original dataset represented *one hospital discharge*, rather than *one unique patient*. In order to avoid introducing bias into our predictive models via the presence of multiple entries for the same patient, we removed all observations for which the number of prior treatment episodes in a drug or alcohol program was greater than 0. This ensured that each discharge entry and all relevant variables were indeed obtained from a unique patient. We additionally removed all entries wherein the patient was discharged from a 24-hour detoxification service, as detoxification and treatment programs are quite distinctive with regards to program settings and goals. Consequently, we focused our analyses on short (< 30 days) and long term (> 30 days) treatment programs that were inpatient, partial hospitalization, or intensive outpatient programs. 

Prior to developing statistical models aimed at predicting treatment completion versus non-completion, we additionally removed predictor variables that were redundant or non-variable across entries. 

* Examples of redundant variables excluded from the dataset include state code, metropolitan area, and census region (redundant with regards to census division) and secondary and tertiary route of drug administration. 
* Predictors that were non-variable across discharge entries and removed from the dataset included year of discharge and number of prior SUD treatment programs participated in. 

Next, we looked at the number of missing values per each predictor variable and removed those with a large number of missing values (>40 % of entries missing data). The following variables were removed for missing data: number of arrests in 30 days prior to admission, DSM diagnosis, detailed not in labor force status, pregnant at time of admission (yes/no), detailed criminal justice referral, health insurance type, and primary source of treatment program payment. 

After finalizing the set of predictor variables, we removed any discharge entries that still had missing values for one or more of the predictors. The final cleaned dataset included 42 predictor variables and 70,009 observations. The table below summarizes relevant treatment program information for these remaining observations: \newline\newline 

```{r Treatment Table, echo=F, fig.height = 5, fig.width = 5, fig.align = "center"}
img1 <-  rasterGrob(as.raster(readPNG("./treatmentInfoPNG.png")), interpolate = FALSE)
grid.arrange(img1, ncol = 1)  
```

```{r Clean 1, eval=FALSE, include=FALSE}
# Remove 24-Hour Detoxes and Ambulatory Detox:
drugdata.cleaned <- drugdata %>% filter(SERVSETD == "4" | SERVSETD == "5" | SERVSETD == "6" | SERVSETD == "7")
```

```{r Clean 2, eval=FALSE, include=FALSE}
# Remove Participants with Prior Treatment:
drugdata.cleaned <- drugdata.cleaned %>% filter(NOPRIOR == 0)
``` 

```{r Clean 3,  eval=FALSE, include=FALSE}
# Remove Columns of Non-Interest, Low Interpretability or Redundancy:
drugdata.cleaned <- drugdata.cleaned %>% dplyr::select(-FREQ2, -FREQ3, -FRSTUSE2, -FRSTUSE3, -ROUTE2, -ROUTE3, -STFIPS, -SUB2, -SUB3, -CBSA, -PMSA, -DSMCRIT, -CASEID, -DISYR, -REGION, - NOPRIOR, -ARRESTS)
```

```{r,  eval=FALSE, include=FALSE}
# Look at NAs per Column:
sapply(drugdata.cleaned, function(x) sum(is.na(x)))
```

```{r Clean 4,  eval=FALSE, include=FALSE}
# Remove Columns with too many NAs:
# DETNLF = 75% NAs
# PREG = 67%
# DETCRIM = 70%
# HLTHINS = 45%
# PRIMPAY = 52%
drugdata.cleaned <- drugdata.cleaned %>% dplyr::select(-DETNLF, -PREG, -DETCRIM, -PRIMPAY, -HLTHINS)
```

```{r Remove NAs,  eval=FALSE, include=FALSE}
# Remove Rows with NAs:
drugdata.cleaned <- na.omit(drugdata.cleaned)
sapply(drugdata.cleaned, function(x) sum(is.na(x))) #check
```

```{r Fix Classes,  eval=FALSE, include=FALSE}
# Look at Variable Class:
for(var in c("AGE", "GENDER","RACE","ETHNIC","MARSTAT", "EMPLOY", "EDUC","VET",
             "LIVARAG", "PRIMINC", "DIVISION", "SERVSETD", "METHUSE", 
             "PSOURCE", "SUB1", "ROUTE1", "FREQ1", "FRSTUSE1", "IDU", "ALCDRUG","PSYPROB")){
      drugdata.cleaned[,var] <- as.factor(drugdata.cleaned[,var])
}

drugdata.cleaned[ , grepl('FLG', names(drugdata.cleaned)) ] <- lapply(drugdata.cleaned[ , grepl('FLG' , names(drugdata.cleaned)) ], factor)    
sapply(drugdata.cleaned, function(x) class(x))
```

```{r Code Treatmeant Completion,  eval=FALSE, include=FALSE}
# RECODE VARIABLE OF INTEREST:
for(num in c(2:8)){
drugdata.cleaned$REASON[drugdata.cleaned$REASON == num] <- 0
}
```

```{r,  eval=FALSE, include=FALSE}
# Look at Within-Variable Variability:
sapply(drugdata.cleaned, function(x) table(x))
```

```{r Save Cleaned Data,  eval=FALSE, include=FALSE}
# Write Out Finalized Dataset:
write.csv(x = drugdata.cleaned, file="./TEDS-D_DrugTreatment_Outcomes_Final.csv", sep=",", row.names = FALSE)
```

### Treatment Success Predictors

The following variables were considered potential predictors of treatment completion versus non-completion for all statistical models:

* Demographic information: Age, sex, race, ethnicity, marital status, years of education, employment status, primary source of income, living arrangement, veteran status, census division, 
* Treatment-related variables: Service setting at discharge, number of days patient waited before entering treatment program, length of treatment stay, principal source of treatment program referral, 
* Substance-related variables: Primary SUD diagnosis, primary route of drug administration, frequency of drug use, age of first drug use, number of substances reported at admission, IV drug use reported at admission, prescribed a pharmacological opioid therapy at admission, alcohol/drug substance use type (alcohol only, drugs only, both), and comorbid psychiatric diagnosis (yes/no). 

The cleaned data additionally included 18 variables pertaining to whether 18 different substances were reported as a substance of use/abuse at admission (i.e. “drug flags” at admission). 

```{r Get Cleaned Data, include=FALSE}
# Read in Cleaned Data
drug.data <- read.csv("./TEDS-D_DrugTreatment_Outcomes_Final.csv", sep=",", header=T)

drug.data <- drug.data %>% rename(TREATCOMPLETE = REASON)

#Change Variable Classes
for(var in c("AGE", "GENDER","RACE","ETHNIC","MARSTAT", "EMPLOY", "EDUC","VET",
             "LIVARAG", "PRIMINC", "TREATCOMPLETE","DIVISION", "SERVSETD", "METHUSE", 
             "PSOURCE", "SUB1", "ROUTE1", "FREQ1", "FRSTUSE1", "IDU", "ALCDRUG","PSYPROB")){
      drug.data[,var] <- as.factor(drug.data[,var])
}

drug.data[ , grepl('FLG', names(drug.data)) ] <- lapply(drug.data[ , grepl('FLG' , names(drug.data)) ], factor)    
```

```{r Sample Characteristics, eval=FALSE, include=FALSE}
#Demographics 
table(drug.data$AGE)
table(drug.data$GENDER)
table(drug.data$RACE)
table(drug.data$MARSTAT)
table(drug.data$EDUC)
table(drug.data$EMPLOY)

#Drug Use Information
table(drug.data$SUB1)
table(drug.data$FRSTUSE1)
table(drug.data$ALCDRUG)

#Treatment Information
table(drug.data$SERVSETD)
table(drug.data$DAYWAIT)
table(drug.data$LOS)
```

In order to reduce the number of statistical predictors, increase model degrees of freedom, and better capture variance across individuals in the sample, a logistic principal components analysis (PCA) was performed on the 18 drug flags. This data reduction strategy allowed for the identification of a small number of principal components that maximally captured variance in drug use at admission. 

Logistic PCA solutions with 1-7 principal components were tested, and a 6 principal component solution was chosen. The 6 principal components identified explained 94% of drug flag variable variance. Principal component scores for the 6 components were generated for each observation, and these 6 scores were used as potential model predictors in addition to the 24 variables described above. The R function `logisticPCA` from package `logisticPCA` was used for this analysis.

```{r PCA Set Up, include=FALSE}
# Logistic Principal Component Analysis for Dimensionality Reduction of Drug Admissions Data
# Input: Drug Flags --> Flags indicate whether each drug was reported as in use at admission (0=no, 1=yes) for 18 drugs

# Step 1. Get drug flag data for input into logistic PCA
pca.data <- drug.data[, c(24:41)]
for(var in c(1:18)){
      pca.data[,var] <- as.numeric(pca.data[,var]) 
}
pca.data$ALCFLG[pca.data$ALCFLG == 1] <- 0
pca.data$ALCFLG[pca.data$ALCFLG == 2] <- 1
pca.data$COKEFLG[pca.data$COKEFLG == 1] <- 0
pca.data$COKEFLG[pca.data$COKEFLG == 2] <- 1
pca.data$MARFLG[pca.data$MARFLG== 1] <- 0
pca.data$MARFLG[pca.data$MARFLG == 2] <- 1
pca.data$HERFLG[pca.data$HERFLG== 1] <- 0
pca.data$HERFLG[pca.data$HERFLG == 2] <- 1
pca.data$METHFLG[pca.data$METHFLG== 1] <- 0
pca.data$METHFLG[pca.data$METHFLG == 2] <- 1
pca.data$OPSYNFLG[pca.data$OPSYNFLG== 1] <- 0
pca.data$OPSYNFLG[pca.data$OPSYNFLG == 2] <- 1
pca.data$PCPFLG[pca.data$PCPFLG== 1] <- 0
pca.data$PCPFLG[pca.data$PCPFLG == 2] <- 1
pca.data$HALLFLG[pca.data$HALLFLG == 1] <- 0
pca.data$HALLFLG[pca.data$HALLFLG == 2] <- 1
pca.data$MTHAMFLG[pca.data$MTHAMFLG== 1] <- 0
pca.data$MTHAMFLG[pca.data$MTHAMFLG == 2] <- 1
pca.data$AMPHFLG[pca.data$AMPHFLG== 1] <- 0
pca.data$AMPHFLG[pca.data$AMPHFLG == 2] <- 1
pca.data$STIMFLG[pca.data$STIMFLG== 1] <- 0
pca.data$STIMFLG[pca.data$STIMFLG == 2] <- 1
pca.data$BENZFLG[pca.data$BENZFLG== 1] <- 0
pca.data$BENZFLG[pca.data$BENZFLG == 2] <- 1
pca.data$TRNQFLG[pca.data$TRNQFLG== 1] <- 0
pca.data$TRNQFLG[pca.data$TRNQFLG == 2] <- 1
pca.data$BARBFLG[pca.data$BARBFLG== 1] <- 0
pca.data$BARBFLG[pca.data$BARBFLG == 2] <- 1
pca.data$SEDHPFLG[pca.data$SEDHPFLG== 1] <- 0
pca.data$SEDHPFLG[pca.data$SEDHPFLG == 2] <- 1
pca.data$INHFLG[pca.data$INHFLG== 1] <- 0
pca.data$INHFLG[pca.data$INHFLG == 2] <- 1
pca.data$OTCFLG[pca.data$OTCFLG== 1] <- 0
pca.data$OTCFLG[pca.data$OTCFLG == 2] <- 1
pca.data$OTHERFLG[pca.data$OTHERFLG== 1] <- 0
pca.data$OTHERFLG[pca.data$OTHERFLG == 2] <- 1
```

```{r PCA Tune, eval=FALSE, include=FALSE}

# Step 2. Tune Parameter K. Choose K=6!
logpca_model_k1_m5 = logisticPCA(pca.data, k = 1, m = 5)
logpca_model_k1_m5$prop_deviance_expl #27%

logpca_model_k2_m5 = logisticPCA(pca.data, k = 2, m = 5)
logpca_model_k2_m5$prop_deviance_expl #53%

logpca_model_k3_m5 = logisticPCA(pca.data, k = 3, m = 5)
logpca_model_k3_m5$prop_deviance_expl #70%

logpca_model_k4_m5 = logisticPCA(pca.data, k = 4, m = 5)
logpca_model_k4_m5$prop_deviance_expl #79%

logpca_model_k5_m5 = logisticPCA(pca.data, k = 5, m = 5)
logpca_model_k5_m5$prop_deviance_expl #84%

logpca_model_k6_m5 = logisticPCA(pca.data, k = 6, m = 5)
logpca_model_k6_m5$prop_deviance_expl #88%

logpca_model_k7_m5 = logisticPCA(pca.data, k = 7, m = 5)
logpca_model_k7_m5$prop_deviance_expl #90%

#Step 3. Tune Parameter M (if m not specified, best m is solved for at given K)
logpca_final_model = logisticPCA(pca.data, k = 6, m=0)
logpca_final_model$prop_deviance_expl #94%
save(logpca_final_model, file="./logpca_final_model.Rdata") #Save out results of PCA
```

```{r PCA Get PCs, include=FALSE}
load("./logpca_final_model.Rdata") #Read in results of PCA

# Step 4. Add PC Scores to drug data 
PCs <- data.frame(logpca_final_model$PCs) 
drug.data$PC1 <- PCs$X1
drug.data$PC2 <- PCs$X2
drug.data$PC3 <- PCs$X3
drug.data$PC4 <- PCs$X4
drug.data$PC5 <- PCs$X5
drug.data$PC6 <- PCs$X6
drug.data <- drug.data %>% dplyr::select(-ends_with("FLG"))
```

## 2.2. Study Sample Characteristics

70,009 unique patients (46,986 male and 23,023 female patients) who were discharged from a substance use disorder treatment program in 2006 were included in this report. Patient ages ranged from 12-55+, and most patients were African American or Caucasian. Comprehensive sample demographics are shown below: \newline

```{r Demographics Table, echo=F, fig.height = 6.7, fig.width = 5, fig.align = "center"}
img1 <-  rasterGrob(as.raster(readPNG("./demoInfoPNG.png")), interpolate = FALSE)
grid.arrange(img1, ncol = 1)  
```

With regards to substance use in the current sample, 42% of the sample had a primary alcohol use disorder, 30% were primarily cannabis users, and 15% of the sample reported a primary cocaine use disorder. About 3% of the sample reported primary use of heroin, and another 3% reported primary use of opioids. Of note, 52% of individuals reported regularly using more than 1 substance at treatment admission. The majority of the study sample reported that their age of first drug use of the drug listed as “primary” was 17 or younger (7% at age 11 or under, 23% at ages 12-14, and 33% at ages 15-17). \newline

```{r Drug Table, echo=F, fig.height = 5.6, fig.width = 5, fig.align = "center"}
img1 <-  rasterGrob(as.raster(readPNG("./drugInfoPNG.png")), interpolate = FALSE)
grid.arrange(img1, ncol = 1)  
```

Comorbid drug and alcohol use disorders were quite prevalent, with 40% of the sample reporting combined alcohol and drug SUDs (35% reported only a drug SUD, and 25% only an alcohol SUD). Interestingly, drug and alcohol disorder comorbidity was higher in the group of individuals that did not complete treatment (43% of this subsample had comorbid drug and alcohol SUDs) as compared to those that completed treatment (35% of this subsample had comorbid SUDs), as was the number of substances reported at admission (59% of treatment non-completers were using 2+ drugs, whereas only 45% of treatment completers were). Overall, substance use in this population of individuals seeking treatment appears to be chronic, complex, and highly comorbid, three factors that increase risk of treatment termination or relapse and that make predicting treatment completion critical.  

## 2.3. Statistical Models

### Training, Testing, and Validation Data Split

In order to test the prediction accuracy of each of the statistical models developed in an unbiased manner (i.e. on data that was *not* used to train the models), the 70,009 observations included in the cleaned TEDS-D dataset were split into:

* Training data cohort (N= 45,000)
* Validation data cohort (N= 5,009) 
* Testing data cohort (N= 20,000)

Each model was trained using the same training data observations, and model classification accuracy was tested on the left out testing data. The smaller validation cohort was used to tune neural network parameters and to report an additional, unbiased testing error for the final model. 

```{r Train and Test, include=FALSE}
#Extract training and testing data
set.seed(256)

N <- nrow(drug.data)
index.train <- sample(N, 45000)
drugdata.train <- drug.data[index.train,]
drugdata.test <- drug.data[-index.train,]

N <- nrow(drugdata.test)
index.validation <- sample(N, 5009)
drugdata.validation <- drugdata.test[index.validation,]
drugdata.test <- drugdata.test[-index.validation,]
```

### Statistical Models

#### Regression-based Approaches

Three types of regression-based approaches were applied in this work. For each regression type, training data were used to fit the model. Misclassification error (MCE), area under the curve (AUC), and Akaike Information Criterion (AIC) were calculated using testing data to assess model performance.

**Full Model Logistic Regression:**
A logistic regression model was fit with all possible predictors (R function `glm`) for the prediction of treatment completion versus non-completion. Beta coefficients were estimated, and the probability of treatment completion was calculated for each patient using the formula: 
$$ P(Y=1 | X) = \frac{e^{\beta_0+\beta_1*X1 ... \beta_N*XN}}{1+e^{\beta_0+\beta_1*X1 ... +\beta_N*XN}}$$
Individual probabilities were used to classify patients into completers (1) and non-completers (0) using a binary cut-off threshold probability=0.5.

```{r GLM Full Model, include=FALSE}
# Step 1. Fit a logistic regression with all possible predictors
glm.allpredictors <- glm(TREATCOMPLETE ~ ., family=binomial, data=drugdata.train) 

# Step 2. Get fitted values and classification for testing data
glm.allpredictors.fittedprobabilities <- predict(glm.allpredictors, drugdata.test, type="response")  #Get probabilities
glm.allpredictors.classify <- ifelse(glm.allpredictors.fittedprobabilities > 0.5, "1", "0") #Classify

# Step 3. Evaluate
testingerror.glm.allpredictors.MCE <- mean(drugdata.test$TREATCOMPLETE != glm.allpredictors.classify) # MCE= 0.3077 
glm.allpredictors.roc <- roc(drugdata.test$TREATCOMPLETE, glm.allpredictors.fittedprobabilities, plot=F) #AUC= 0.756
```

**Logistic Regression with Backwards Selection**
In an effort to create a more parsimonious and more interpretable model by removing predictors, `stepAIC` -- an alternative to the function `bestglm` for model selection -- was employed perform iterative backwards selection, stopping with a local minimum AIC is reached. The final model included 26 variables. 

```{r Step AIC, eval=FALSE, include=FALSE}
# Step 1. Use stepAIC to iteratively delete the factor with lowest AIC
pacman::p_load(MASS)

glm.allpredictors_step = stepAIC(glm.allpredictors, direction = 'backward') 

# Step 2. Fit this model!
glm.reduced <- glm(TREATCOMPLETE ~ AGE + GENDER + RACE + ETHNIC + MARSTAT + EDUC + EMPLOY + VET + LIVARAG + PRIMINC + DIVISION + SERVSETD + METHUSE + DAYWAIT + LOS + PSOURCE + SUB1 + FREQ1 + FRSTUSE1 + NUMSUBS + IDU + ALCDRUG + PSYPROB + PC3 + PC4 + PC6, family=binomial, data=drugdata.train)
Anova(glm.reduced)

save(glm.reduced, file="./Regression_stepAICmodel.Rdata") # Save out final model
```

```{r Step AIC prediction, eval=FALSE, include=FALSE}

load("./Regression_stepAICmodel.Rdata")

# Step 3. Get fitted values and classification for testing data
glm.reduced.fittedprobabilities <- predict(glm.reduced, drugdata.test, type="response")  #Get probabilities
glm.reduced.classify <- ifelse(glm.reduced.fittedprobabilities > 0.5, "1", "0") #Classify

# Stem 4. Evaulate the model
testingerror.glm.reduced.MCE <- mean(drugdata.test$TREATCOMPLETE != glm.reduced.classify) # MCE= 0.30705 
glm.reduced.roc <- roc(drugdata.test$TREATCOMPLETE, glm.reduced.fittedprobabilities, plot=F) #AUC= 0.756
```

**Penalized Regression:**
Finally, we used elastic net regression, which, by adjusting the alpha, combines the shrinkage and sparsity of LASSO with the uniqueness and tolerance of collinearity of ridge regression. In other words, the elastic net penalized regression (`cv.glmnet` function) provides some balance between LASSO and ridge regressions. Our elastic net model was fit with coefficients extracted from both `lambda.min` and `lambda.1se`. Of these, the model with the lowest MCE and highest AUC was selected. This final model included 28 variables. 

```{r Elastic Net, eval=FALSE, include=FALSE}
# Use elastic net to find important variables (training data)
# Step 1. Format data

y <- drugdata.train[, 15]
x <- model.matrix(TREATCOMPLETE~.,data=drugdata.train)[,-15]
set.seed(10)

fit.cv <- cv.glmnet(x, y, alpha=0.99, family="binomial", nfolds = 10, type.measure = "deviance")  
plot(fit.cv)
fit.MCE.cv <- cv.glmnet(x, y, alpha=0.99, family="binomial", nfolds = 10, type.measure = "class")  
plot(fit.MCE.cv)
fit.auc.cv <- cv.glmnet(x, y, alpha=0.99, family="binomial", nfolds = 10, type.measure = "auc")  
plot(fit.auc.cv)

# We could chose lambda=fit1.cv$lambda.1se, lambda.min or any values of the lambda near the min points.
# Step 2. Look at lambda.min for minimizing deviance (because deviance is most important when considering classification/prediction):
coef.min <-coef(fit.cv, s="lambda.min") 
coef.min <- coef.min[which(coef.min !=0), ]
rownames(as.matrix(coef.min)) 

# AGE, GENDER, RACE, ETHNIC, MARSTAT, EDUC, EMPLOY, VET, LIVARAG, PRIMINC, DIVISION, SERVSETD, METHUSE, DAYWAIT, LOS, PSOURCE, SUB1, ROUTE1, FREQ1, FRSTUSE1, NUMSUBS, IDU, ALCDRUG, PSYPROB, PC1, PC2, PC3, PC5, PC6 (29 vars)
# Step 3. Look at lambda.1se for minimizing deviance:
coef.1se <- coef(fit.cv, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] 
rownames(as.matrix(coef.1se)) 

# AGE, GENDER, RACE, ETHNIC, MARSTAT, EDUC, EMPLOY, VET, LIVARAG, PRIMINC, DIVISION, SERVSETD, METHUSE, DAYWAIT, LOS, PSOURCE, SUB1, ROUTE1, FREQ1, FRSTUSE1, NUMSUBS, IDU, ALCDRUG, PSYPROB, PC1, PC2, PC3, PC6 (28 vars)
# Step 4. Re-fit logistic regression using `glm()` with the variables obtained from LASSO above.
# Penalized regression model #1 (lambda.min)
P.Regression.1 <- glm(TREATCOMPLETE~AGE + GENDER + RACE + ETHNIC + MARSTAT + EDUC + EMPLOY + VET + LIVARAG + PRIMINC + DIVISION + SERVSETD + METHUSE + DAYWAIT + LOS + PSOURCE + SUB1 + ROUTE1 + FREQ1 + FRSTUSE1 + NUMSUBS + ALCDRUG + IDU + PSYPROB + PC1 + PC2 + PC3 + PC5 + PC6, family=binomial, data=drugdata.train)
Anova(P.Regression.1)

# Penalized regression model #2 (lambda.1se)
P.Regression.2 <- glm(TREATCOMPLETE~AGE + GENDER + RACE + ETHNIC + MARSTAT + EDUC + EMPLOY + VET + LIVARAG + PRIMINC + DIVISION + SERVSETD + METHUSE + DAYWAIT + LOS + PSOURCE + SUB1 + ROUTE1 + FREQ1 + FRSTUSE1 + NUMSUBS + ALCDRUG + IDU + PSYPROB + PC1 + PC2 + PC3 + PC6, family=binomial, data=drugdata.train)
Anova(P.Regression.2)

# Step 5. Get fitted values and classification for testing data
P.Regression.1.fittedprobabilities <- predict(P.Regression.1, drugdata.test, type="response")  #Get probabilities lambda.min
P.Regression.1.classify <- ifelse(P.Regression.1.fittedprobabilities > 0.5, "1", "0") #Classify
P.Regression.2.fittedprobabilities <- predict(P.Regression.2, drugdata.test, type="response")  #Get probabilities lambda.1se
P.Regression.2.classify <- ifelse(P.Regression.2.fittedprobabilities > 0.5, "1", "0") #Classify

# Step 6. Evaulate the lambda.min and lambda.1se models
testingerror.P.Regression.1.MCE <- mean(drugdata.test$TREATCOMPLETE != P.Regression.1.classify) # MCE= 0.3071 
P.Regression.1.roc <- roc(drugdata.test$TREATCOMPLETE, P.Regression.1.fittedprobabilities, plot=F) #AUC= 0.7562
testingerror.P.Regression.2.MCE <- mean(drugdata.test$TREATCOMPLETE != P.Regression.2.classify) # MCE= 0.3076 
P.Regression.2.roc <- roc(drugdata.test$TREATCOMPLETE, P.Regression.2.fittedprobabilities, plot=F) #AUC= 0.7562
# lambda.1se is slightly better, let's save this out:
save(P.Regression.2, file="./penalizedRegressionModel.Rdata") # Save out penalized regression model
```

#### Random Forest

In considering the limitations of logistic regression, we investigated the potential benefit of data-driven classification with the R function `randomForest`.

Random forest provides **many layers of randomness** such that each tree is produced from a random sample of cases and split at a random sample of predictors. As such, this partitioning process not only avoids snooping/polluting the data, but also enables a sort of model selection even in instances where p is large. Perhaps more importantly, because different sets of predictors are evaluated for different splits a wide variety of mean functions are evaluated, each potentially constructed from rather different basis functions. This means we get a better fit, a **less restricted approximation of the true response surface**, and, resultantly, a smaller bias. Random forest also incorporates bagging -- the idea that averaging across trees increases stability and that aggregating fitting values across trees makes them more **independent** while simultaneously reducing testing error. 

Specifically, we used the training dataset to tune both `mtry` and `ntree` parameters, settling on an `mtry` of 5 (approximately equal to the suggested $\sqrt{p}$ for classification trees) and `ntree` of 400. As such, the final random forest model included 400 trees; for which each node was split via one of 5 randomly selected variables. To assess the model's predictive accuracy, not only was OOB error assessed, but probabilities and responses were fitted using the testing data as to most appropriately report MCE and AUC. Sensitivity, specificity, as well as positive and negative prediction error were also assessed. 

```{r Build RF, eval=FALSE, include=FALSE}
# Tuning mtry
set.seed(332)
rf.error.p <- 1:30  # Set up a vector of length 21
for (p in 1:30)  # Repeat the following code inside { } 21 times
{
  fit.rf.tuning <- randomForest(TREATCOMPLETE~., drugdata.train, mtry=p, ntree=250)
  rf.error.p[p] <- fit.rf.tuning$err.rate[250]  # collecting OOB MSE based on 250 trees
  print(rf.error.p[p])
}
rf.error.p
# Plotting the results:
plot(1:30, rf.error.p, pch=16,
     xlab="mtry",
     ylab="OOB mse of mtry")
lines(1:30, rf.error.p)
# Fit model with mtry = 5
fit.rf.mtrytune<- randomForest(TREATCOMPLETE ~ ., data = drugdata.train, mtry = 5, importance = T, ntree=250)
# How well does this mtry predict?
fit.rf.mtrytune.pred <- predict(fit.rf.mtrytune, drugdata.test, type="prob")  
fit.rf.mtrytune.pred.y <- predict(fit.rf.mtrytune, drugdata.test, type="response")
fit.rf.mtrytune.train.err <- mean(drugdata.test$TREATCOMPLETE != fit.rf.mtrytune.pred.y) # 0.26655
# Tuning ntree parameter:
fit.rf.ntree<- randomForest(TREATCOMPLETE ~ ., data = drugdata.train, mtry = 5, importance = T, ntree=500)
plot(fit.rf.ntree)
# Growing the forest with optimized parameters
fit.rf <- randomForest(TREATCOMPLETE ~ ., data = drugdata.train, mtry = 5, importance = T, ntree=400)
#save(fit.rf, file="./randomForest_model.Rdata") # Save out final model
save(fit.rf, file="./randomForest_model_new.Rdata") # Save out final model
```

```{r Predict RF, include=FALSE}
load("./randomForest_model_new.Rdata")

# Can better look at model perfomance in testing data
# Prediction using testing data
fit.rf.test.pred <- predict(fit.rf, newdata=drugdata.test, type="prob")  # Output the prob of "0" and "1"
fit.rf.test.pred.y <- predict(fit.rf, newdata=drugdata.test, type="response")
fit.rf.train.err <- mean(drugdata.test$TREATCOMPLETE != fit.rf.test.pred.y) # 0.26445
fit.rf.test.roc <- roc(drugdata.test$TREATCOMPLETE, fit.rf.test.pred[,2], plot=TRUE, col="red") 
pROC::auc(fit.rf.test.roc) # 0.8118 (!!!)
# Confusion matrix
confusionMatrix(fit.rf.test.pred.y, drugdata.test$TREATCOMPLETE, positive = "1") # Switching the positive class so that 1 (treatment completion) is positive
# Sensitivity: 77%
# Specificity: 71%
# Positive Pred: 70%
# Negative Pred: 78%
```

#### Boosting

Boosting is an extension of random forest, but rather than building a forest of *independent* trees using a set number of randomly chosen predictors per tree, each tree built during the boosting process attempts to *improve the prediction performance of the previously generated tree* (i.e. correct previous tree errors). As such, boosted trees are *non-independent*. 

Boosting models improve probability and classification predictions by using an ensemble of decision trees that were generated (and iteratively improved upon) by modifying the weights of individual observations in the training data. More specifically, with boosting, trees fitted initially from equally or randomly weighted observations are improved upon for observations that were misclassified by increasing the weight of the misclassified observations. Over many iterations, weak learning trees with small depth can thus be converted into strong learners by changing input observation weights and optimizing a specified metric (here, accuracy/AUC). Of note, whereas in random forest all generated trees are weighted equally when generating final predictions, trees built during boosting are given different weights in the final predictive model.

Here, the functions `trainControl`, `train`, and `gbm` from packages `caret` and `gbm` were used to develop an optimized boosting tree model. 

* Boosting parameters were chosen by testing interaction depths of 2, 4, 6, and 8 and tree numbers from 500 to 1,000 (in increments of 100 trees), using a set shrinkage of 0.01 and a minimum of 20 observations per node. 
* Parameters were tested using training data and cross-validation with five folds. 
* The finalized parameters were: an interaction depth of 8, 1,000 trees, a shrinkage of 0.01 and a minimum of 20 observations per node.

```{r ADABoost, eval=FALSE, include=FALSE}
# Step 1. Format Data
x.train <- (drugdata.train[,-(15)]) 
y.train <- as.numeric(drugdata.train[,15]) -1
y.train[y.train == 0] = -1

x.test <- (drugdata.test[,-(15)])
y.test <- as.numeric(drugdata.test[,15]) -1
predictions

# Step 2. Boost
## Tune Parameters
###Tune tree depth (try 3, 5, 6, 7, 8, 10). We choose a tree depth of 6 to lower testing error!
boost.3.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 3, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.3.30, data.matrix(x.train))
train_err.3.30 <- mean(y.train != yhat_train_ada) #0.2944
yhat_test_ada <- predict(boost.3.30, data.matrix(x.test))
test_err.3.30 <- mean(y.test != yhat_test_ada) #0.301

boost.5.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 5, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.5.30, data.matrix(x.train))
train_err.5.30 <- mean(y.train != yhat_train_ada) #0.275
yhat_test_ada <- predict(boost.5.30, data.matrix(x.test))
test_err.5.30 <- mean(y.test != yhat_test_ada) #0.2907

boost.6.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 6, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.6.30, data.matrix(x.train))
train_err.6.30 <- mean(y.train != yhat_train_ada) #0.2578
yhat_test_ada <- predict(boost.6.30, data.matrix(x.test))
test_err.6.30 <- mean(y.test != yhat_test_ada) #0.28425

boost.7.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 7, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.7.30, data.matrix(x.train))
train_err.7.30 <- mean(y.train != yhat_train_ada) #0.242
yhat_test_ada <- predict(boost.7.30, data.matrix(x.test))
test_err.7.30 <- mean(y.test != yhat_test_ada) #0.2863

boost.8.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 8, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.8.30, data.matrix(x.train))
train_err.8.30 <- mean(y.train != yhat_train_ada) #0.2165
yhat_test_ada <- predict(boost.8.30, data.matrix(x.test))
test_err.8.30 <- mean(y.test != yhat_test_ada) #0.292

boost.10.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 10, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.10.30, data.matrix(x.train))
train_err.10.30 <- mean(y.train != yhat_train_ada) #0.1148
yhat_test_ada <- predict(boost.10.30, data.matrix(x.test))
test_err.10.30 <- mean(y.test != yhat_test_ada) #0.2979

#Tune n_rounds

boost.6.50 <- adaboost(data.matrix(x.train), y.train, tree_depth = 6, n_rounds = 50,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.6.50, data.matrix(x.train))
train_err.6.50 <- mean(y.train != yhat_train_ada) #0.247
yhat_test_ada <- predict(boost.6.50, data.matrix(x.test))
test_err.6.50 <- mean(y.test != yhat_test_ada) #0.28422
```

```{r Final Adaboost, eval=FALSE, include=FALSE}
#FINAL BOOSTING TREES MODEL
x.train <- (drugdata.train[,-(15)]) 
y.train <- as.numeric(drugdata.train[,15]) -1
y.train[y.train == 0] = -1

x.test <- (drugdata.test[,-(15)])
y.test <- as.numeric(drugdata.test[,15]) -1
y.test[y.test == 0] = -1

#boost.6.100 <- adaboost(data.matrix(x.train), y.train, tree_depth = 6, n_rounds = 500,
  #                  verbose = FALSE,
   #                control = NULL)
#save(boost.6.100, file="./final_boosting_model.Rdata") #Save out final model
load("./final_boosting_model.Rdata")

# Step 3. Evaluate

#yhat_train_ada <- predict(boost.6.100, data.matrix(x.train))
#train_err.6.100 <- mean(y.train != yhat_train_ada) #0.2231 = TRAINING ERROR 0.2231

#MCE
yhat_test_ada <- predict(boost.6.100, data.matrix(x.test))
testingerror.boosting.6.100.MCE <- mean(y.test != yhat_test_ada) # MCE = 0.27945 = FINAL TESTING ERROR FOR BOOSTING TREES

#AUC
boosting.fitted <- predict(boost.6.100, data.matrix(drugdata.test), type="response") 
boosting.fitted[boosting.fitted == -1] = 0
boosting.roc <- roc(drugdata.test$TREATCOMPLETE, boosting.fitted, plot=F) #AUC = 0.72
```

```{r GBM Boosting Train, eval=FALSE, include=FALSE}
# Step 1. Format Data
x.boost.train <- drugdata.train[,-15]
y.boost.train <- make.names(drugdata.train[,15])
x.boost.test <- drugdata.test[,-15]
y.boost.test <- drugdata.test[,15]

# Step 2: Tune Boosting Parameters
fitControl <- trainControl(method = "cv",
                           number = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)
gbmGrid <-  expand.grid(interaction.depth = c(18, 20), 
                        n.trees = (18:22)*100, 
                        shrinkage = 0.01,
                        n.minobsinnode = 20)
GBM.boosting.model <- caret::train(x.boost.train,y.boost.train, 
                 method='gbm', 
                  trControl=fitControl,
                  tuneGrid = gbmGrid,
                  metric = "ROC")
print(GBM.boosting.model)
#save(GBM.boosting.model, file="./GBM_Boosting_Model.Rdata") #Save out final model
```

```{r GM Boosting Predict, include=FALSE}
x.boost.train <- drugdata.train[,-15]
y.boost.train <- make.names(drugdata.train[,15])
x.boost.test <- drugdata.test[,-15]
y.boost.test <- drugdata.test[,15]

load("./GBM_Boosting_Model.Rdata")

# Step 3. Evaluate
predictions <- predict(object=GBM.boosting.model, x.boost.test, type='prob')
boosting.probabilities <- predictions[,2]
boosting.classify <- ifelse(boosting.probabilities > 0.5, "1", "0")
testingerror.boost.MCE <- mean(drugdata.test$TREATCOMPLETE != boosting.classify) #TESTING ERROR = 0.2698
gbmboost.roc <- roc(drugdata.test$TREATCOMPLETE, boosting.probabilities, plot=F) #AUC = 0.8075
importantpredictors <- summary(GBM.boosting.model)
```

```{r Boost Predictors, echo=FALSE}
# Boosting 15 MOST IMPORTANT PREDICTORS
importantpredictors.df <- data.frame(importantpredictors)
rownames(importantpredictors.df) <- c()
importantpredictors.df <- importantpredictors.df %>% rename(Variable = var)
importantpredictors.df <- importantpredictors.df %>% rename(Relative_Influence = rel.inf)
#importantpredictors.df[1:10,]
```

#### Deep Learning (Neural Net)

To create a more sophisticated, more complex model allowing for multiple levels of non-linearity, we built a deep learning network (also known as a neural network model). 

```{r eval=FALSE, fig.align='center', include=FALSE, out.width='40%'}
knitr::include_graphics(c("./NeuralNetExample.png"))
```

In deep learning, these layered representations are (almost always) learned via models called *neural networks*. Each layer in the network consists of a number of individual units, called "neurons", and each neuron has the ability to perform a simple function on its inputs. In a dense layer, all of the input neurons connect to all of the output neurons with certain weights. During the course of training this network model, various parameters are adjusted, including the weights of the connections and the biases (effectively the intercept terms for the linear combinations).

Here, we use the package 'keras' to develop the deep learning model. 

We chose to use a network structure that contained layers of varying size and the "relu" activation function, which transforms negative values into zeros. In order to avoid over-fitting the model, we added dropout layers between the "relu" layers. For the final layer, we chose to use a sigmoid activation function, which transforms the input information into a prediction from 0 to 1. In order to train the model, we chose to use 'rmsprop' as our optimizer logarithm, binary cross-entropy as our loss function (which the model will minimize), and the fraction of correctly classified patients as our metric of success. We first tuned the hyperparameters of the model (number of patients trained upon before changing the weights and total number of cycles through the data) by running our model on the training data and "validating" it on the validation data after every parameter change. In terms of the batch size, we wanted to train on a large enough set each time that the parameters would be tuned in the right direction. At the same time, having a very large batch size misses the point of dividing the data into batches, which is to avoid getting stuck in local maxima. To solve this optimization problem, we decided on a batch size of 200.

```{r Neural Net Tuning, eval=FALSE, include=FALSE}

# Step 1. Define the Model
model <- keras_model_sequential() %>%
  layer_dense(units = 60, activation = "relu", input_shape = c(30)) %>%
  layer_dropout(0.05) %>%
  layer_dense(units = 10, activation = "relu") %>% 
  layer_dropout(0.05) %>%
  layer_dense(units = 40, activation = "relu") %>% 
  layer_dropout(0.05) %>%
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

# Step 2. Compile the Model
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

#Step 3. Train the Model
## Determine Epoch number using training and validation data sets

### Format Data
x_train <- as.matrix(drugdata.train[,-15])
y_train <- as.matrix(drugdata.train[,15])
x_validate <- as.matrix(drugdata.validation[,-15])
y_validate <- as.matrix(drugdata.validation[,15])
x_test <- as.matrix(drugdata.test[,-15])
y_test <- as.matrix(drugdata.test[,15])

neuralnet.tune <- model %>% fit(x_train, 
                                y_train,
                                epochs = 50, 
                                batch_size = 200, 
                                validation_data = list(x_validate, y_validate)
                                )
```

```{r Neural Net, include=FALSE}
# Step 4. Refit Model with Chosen Parameters
set.seed(1)
# FINAL MODEL
model <- keras_model_sequential() %>%
  layer_dense(units = 60, activation = "relu", input_shape = c(30)) %>%
  layer_dropout(0.05) %>%
  layer_dense(units = 10, activation = "relu") %>% 
  layer_dropout(0.05) %>%
  layer_dense(units = 40, activation = "relu") %>% 
  layer_dropout(0.05) %>%
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

x_train <- as.matrix(drugdata.train[,-15])
y_train <- as.matrix(drugdata.train[,15])
x_test <- as.matrix(drugdata.test[,-15])
y_test <- as.matrix(drugdata.test[,15])

NN.allpredictors <- model %>% fit(x_train, y_train, epochs = 25, batch_size = 200)

#save(NN.allpredictors, file="./Deep_Learning_Model.Rdata")

# Step 5. Evaluate 
## AUC 
results <- model %>% evaluate(x_test, y_test)

## MCE
MCE.NN <- 1-results$acc # MCE = 0.31
MCE.NN

#AUC Plot
NN_predictions <- predict(object = model, x = as.matrix(x_test)) %>% as.vector()
NN.roc <- roc(drugdata.test$TREATCOMPLETE, NN_predictions, plot=F) #AUC = 0.75
```

# 3. Results & Model Comparisons

## 3.1 Regression-based Approaches

In considering the balance between MCE and AUC, a logistic regression fit following automated stepwise, backwards tuning of AIC (`stepAIC`) was our best regression-based approach (MCE = 0.308; AUC = 0.756). However, it provided little benefit over regular `glm` or penalized regression (i.e., elastic net). Moreover, 20 of the 26 variables included in the regression-based model were significant at the p = 0.001 level. While this means that the TEDS-D dataset includes many helpful and informative predictors of treatment success, it also means that the final model is not particularly parsimonious and, therefore, not particularly relevant for informing targeted policy or intervention. Furthermore, while the regression-based models are easily implemented and easily interpreted, the assumptions that need to be met for regressions are strong and -- since the true data generating model is unknown -- these assumptions are usually violated. 

## 3.2 Random Forest

Prediction performance was substantially increased with respect to `randomForest`. Not only were we able to reduce MCE from 0.308 to 0.266, but we were also able to increase AUC from 0.756 to 0.812. Of note, the final random forest model was also particularly balanced:

* Sensitivity: 77%; Specificity: 71%; Positive prediction error: 70%; Negative prediction error: 78%

As such, the model bosts rather impressive differentiation between patients who completed their sessions and those who did not.

## 3.3 Boosting 

For our boosting model, despite the expected advantage due to attempts by the boosting process to improve the prediction performance of the previously generated tree, the AUC (0.807) was comparable to (in fact, a little lower than) that of `randomForest`. Of note, both `randomForest` and boosting identified the variables `DIVISION` and `LOS` as most predictive of treatment outcomes. 

```{r Boosting Predictor Plot, echo=FALSE, fig.height=4.7, fig.width=5}
# BOOSTING PREDICTOR RELATIVE INFLUENCE GRAPH
plot(varImp(GBM.boosting.model,scale=F, xlab="Variable Importance"), , col="yellow")
```

## 3.4 Deep learning

As seen in the plot below, our deep learning model revealed a decreasing loss (and ACC) over the selected 25 epochs. 

The was able to correctly classify 68.5% of the data- significantly above average, but certaintly not remarkable. MCE was 0.315 and AUC of 0.75 --  falling below those of both random forest and boosting. 

```{r, out.width='55%', echo=FALSE}
knitr::include_graphics(c("./NN_Performance.png"), dpi = 300)
```

```{r PLOT ROCs,fig.height=5, fig.width=8, echo=F}
par(mfrow=c(1,1))
plot(1-NN.roc$specificities, NN.roc$sensitivities, col="deepskyblue4", pch=16, cex=0.4, xlab= "False Positives", ylab="Sensitivities", font.lab=2, main="Model Comparisons: ROC Curves", font.main=2,
text(.75, .2, paste("\nModel 1: Random Forest AUC=", round(pROC::auc(fit.rf.test.roc),3),
  "\nModel 2: Boosting AUC=", round(pROC::auc(gbmboost.roc),3),
                "\nModel 3: stepAIC GLM AUC=",round(pROC::auc(glm.allpredictors.roc),3), 
                "\nModel 4: Neural Net AUC=",round(pROC::auc(NN.roc),3))))

points(1-fit.rf.test.roc$specificities, fit.rf.test.roc$sensitivities, col="springgreen4", pch=16, cex=0.4)

points(1-glm.allpredictors.roc$specificities, glm.allpredictors.roc$sensitivities, col="skyblue2", pch=16, cex=0.4)

points(1-gbmboost.roc$specificities, gbmboost.roc$sensitivities, col="yellow", pch=16, cex=0.4)


legend(0, 1, legend=c("Model 1", "Model 2",  "Model 3", "Model 4"),
       col=c("springgreen4",  "yellow", "skyblue2", "deepskyblue4"), lty=1:1, cex=.9)

lines(1-fit.rf.test.roc$specificities, fit.rf.test.roc$sensitivities, lwd=2, col="springgreen4")
lines(1-gbmboost.roc$specificities, gbmboost.roc$sensitivities, lwd=2, col="yellow")
lines(1-glm.allpredictors.roc$specificities, glm.allpredictors.roc$sensitivities, lwd=2, col="skyblue2")
lines(1-NN.roc$specificities, NN.roc$sensitivities, lwd=2, col="deepskyblue4")
```

# 4. Final Model

Using MCE and AUC as primary model selection criteria (plotted above), we chose random forest as our final model. 

In order to further assess performance of this final model and to get a second, honest/unbiased estimate of testing error, we fit the chosen `mtry` and `ntree` parameters with the reserved validation data. While the MCE for this validation-based fit was comparable to the original model, the AUC actually increased to 0.814 -- confirming the generalizability of the model.

```{r RF validation fit, eval=FALSE, include=FALSE}
# Growing the forest with optimized parameters
fit.rf.validate <- randomForest(TREATCOMPLETE ~ ., data = drugdata.validation, mtry = 10, importance = T, ntree=400)

#save(fit.rf, file="./randomForest_model.Rdata") # Save out final model
save(fit.rf, file="./randomForest_model_validation.Rdata") # Save out final model

# Prediction using validation data
fit.rf.validate.pred <- predict(fit.rf, newdata=drugdata.validation, type="prob")  # Output the prob of "0" and "1"
fit.rf.validate.pred.y <- predict(fit.rf, newdata=drugdata.validation, type="response")
fit.rf.validate.err <- mean(drugdata.validation$TREATCOMPLETE != fit.rf.validate.pred.y) # 0.2641246

fit.rf.validate.roc <- roc(drugdata.validation$TREATCOMPLETE, fit.rf.validate.pred[,2], plot=TRUE, col="red") 
pROC::auc(fit.rf.validate.roc) # 0.8136 (!!!)
```

After selecting the desired model we turn to interpreting its results. There are two main tools we will use to understand the model's output: Variable importance plots and partial dependency plots.

## Variable Importance Plots

One approach to using `randomForest` to get measures of predictor importance is to record the decrease in the fitting measure (e.g. Gini index; mean squared error) each time a given variable is used to define a split. In this sense, the sum of these reductions for a given tree becomes a measure of the importance of that variable when that tree is grown. In the context of random forest, this measure of variable importance can be averaged over the set of trees. 

In the plots below, reduction in prediction accuracy is shown on the horizontal axis. As such, we can see reductions in prediction accuracy for successful treatment completion (left) and non-successful treatment completion (right) when each predictor is in turn randomly shuffled. \newline\newline

```{r RF Var Importance, echo = FALSE}
# Variable importance plots
par(mfrow=c(1,2))
varImpPlot(fit.rf, type=1, scale=F, class="1",
           main = "Importance Plot for Treatment Completion",  cex=.65, pch=19, color="springgreen4") 
varImpPlot(fit.rf, type=1, scale=F, class="0",
           main = "Importance Plot for Non-Completion", cex=.65, pch=19, color="springgreen4") 
```


Specifically, we note that two variables `LOS` (length of stay) and `DIVISION` (census-based geographic logcation) are most predictive of both treatment completion and treatment non-completion. 

Factors like `SERVSETD` (service setting at discharge), `PSOURCE` (principal source of referral), `FREQ1` (frequency of use), `PSYCPROB` (psychiatric problems/symptoms/diagnoses), `EMPLOY` (employment status at time of admission), and `PRIMINC` (source of income/support) are most predictive of successful treatment completion. On the other hand, factors such as `SUB1` (substance problem code),`AGE` (age at time of admission), `FRSTUSE1` (age at first substance use), and `DAYWAIT` (days spent waiting for admission) were most predictive of cases where treatment was not completed. 

## Partial Dependence Plots

For tree-based approaches, these dependence plots show the average relationship between a given input and the response within the fixed, joint distribution of the other inputs. In other words, these plots show how what what the proportion for a particular classification outcome would be for different values of the variable of interest.

Taking this approach for `LOS`, we see that the longer a patient stays in the treatment program, the higher their log odds of completing it becomes. \newline\newline


```{r RF Figures, echo=FALSE, fig.height=2.6, fig.width=7}
# Partial dependence plots
# LOS:
partimp1 <- partialPlot(fit.rf, pred.data=drugdata.test, x.var=LOS, rug=T, which.class="1", plot=F)
scatter.smooth(partimp1$x, partimp1$y, span=1/3, xlab= "Length of Stay", ylab= "Log Odds of Completion", main = "Partial Dependence Plot for LOS on Treatment Completion", col = "springgreen4", pch=19, cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)
```


For `DIVISION`, we begin to see a clear effect of geographic location. Whereas being in treatment in New England (1), Central North West (4), or South Atlantic (5), seems to correspond to greater log odds of treatment completion for patients, being in treatment in -- for example -- the North East (3) or the Mountains (8) has the opposite effect. Intuitively, this makes sense as we consider factors like the prevalence and concentration of drugs in the North East counteracting treatment seeking and, perhaps, effecting relapse. 

An interesting and intuitive relationship is present for `EMPLOY`, too. Having a full-time job (1) as you enter treatment is associated with increased log odds of completing the program whereas log odds of completion fall for patients who enter unemployed (3) or not in the labor force (4). \newline\newline


```{r echo=FALSE, fig.height=2.6, fig.width=7}
par(mfrow=c(1,2))

partialPlot(fit.rf, pred.data=drugdata.test, x.var=DIVISION, rug=T, which.class="1", 
                        main = "DIVISION and Treatment Completion", 
                        xlab= "Census Division", ylab= "Log Odds Completion", ylim = c(-0.3, 0.2), cex.main=0.75, cex.lab=0.75, cex.axis=0.75)

partialPlot(fit.rf, pred.data=drugdata.test, x.var=EMPLOY, rug=T, which.class="1", 
                        main = "EMPLOY and Treatment Completion", 
                        xlab= "Employment Status", ylab= "Log Odds Completion", ylim = c(-0.2, 0.2),  cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)

```


For `PRIMINC` (primary source of income), we also see the expected relationship -- Patients earning wages/salary (1) having higher log odds of treatment completion whereas log odds are decreased for patients reliant on public assistance (2) or retirement/pension/disability (3). 

In addition to economic support, social networks appear to play a role in treatment completion. Here, we see the effects of `PSOURCE` (principal referral source). Interestingly, patients enrolled by themselves or by one other individual (1), healthcare provider (3) or member of the community (6) have lower log odds of completing. On the otherhand, referrals by drug/alcohol abuse mentors and providers (2), schools (4), employers (5), or the court/criminal justice system (6) have more positive effects. \newline\newline


```{r echo=FALSE, fig.height=2.6, fig.width=7}
par(mfrow=c(1,2))

partialPlot(fit.rf, pred.data=drugdata.test, x.var=PRIMINC, rug=T, which.class="1", 
                        main = "PRIMINC and Treatment Completion", 
                        xlab= "Primary Insurance", ylab= "Log Odds Completion", ylim = c(-0.2, 0.2), cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)

partialPlot(fit.rf, pred.data=drugdata.test, x.var=PSOURCE, rug=T, which.class="1",  
                        main = "PSOURCE and Treatment Completion",
                        xlab= "Referral Source", ylab= "Log Odds Completion", ylim = c(-0.2, 0.1), cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)
```


Looking at predictors more directly related to substance use, we see that the drug of choice (`SUB1`) tends to have some effect on treatment outcomes. For example, individuals primarily using drugs like PCP (8), hallucinogens (9), methamphetamine/amphetamine (10/11), benzodiazepines (13), or inhalants (17) have higher log odds of treatment completion. Interestingly, drugs like cocaine (3), marijuana (4), and heroin (5) are associated with lower log odds of success. Perhaps this is due to the greater, more wide-spread availability of drugs like marijuana or cocaine such that participants see less value in sticking out the treatment plan if they know the drug will still be there when they return home.

Relatedly, the frequency of drug use (`FREQ1`) is also tightly related to success/failure of treatment. Specifically, individuals who had no use in the month prior to admission (1) displayed much higher log odds of completion than individuals who came into treatment using 3-6 times in the last week (4) or daily (5). Clinically, this checks out as the more rapidly and more frequently a patient is going through the binge/intoxication, withdrawal/negative affect, anticipation/craving, relapse cycle, the more difficult it become to break free from the chemical imbalanced induced by a hijacking of the brain's dopamine (neurotransmitter) system. \newline\newline


```{r echo=FALSE, fig.height=2.6, fig.width=7}
par(mfrow=c(1,2))

partialPlot(fit.rf, pred.data=drugdata.test, x.var=SUB1, rug=T, which.class="1", main = "SUB1 and Treatment Completion", xlab= "Substance Problem Code",ylab= "Log Odds Completion", ylim = c(-0.1, 0.2), cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)

partialPlot(fit.rf, pred.data=drugdata.test, x.var=FREQ1, rug=T, which.class="1", 
                        main = "FREQ1 and Treatment Completion", 
                        xlab= "Frequency of Use at Admission", ylab= "Log Odds Completion", ylim = c(-0.2, 0.2), cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)

```


Finally, our model suggests clear age-based effects on treatment completion. We can see this on two fronts -- age at first drug use (`FRSTUSE1`) and age at treatment admission (`AGE`). 

We see an obvious relationship between `FRSTUSE1` and treatment success such that the younger an individual was when they began using drugs/alcohol (e.g. 1 --> 11 years old vs. 12 --> 55 and older), the less likely they are to complete the program. This relates back to the progression of addiction as a brain disease. The longer you suffer, the less likely you are to be able to return to self-sufficient, drug-free functioning. 

However, the plot of `AGE` and log odds of treatment completion offers some hope. Here, we see that if a patient seeks treatment at these younger ages, their odds of finishing said treatment are positive. This suggests that it's less of game of age per-say and more dependent on the length of the time window between drug use initiation and treatment seeking. \newline\newline


```{r echo=FALSE, fig.height=2.6, fig.width=7}

par(mfrow=c(1,2))

partialPlot(fit.rf, pred.data=drugdata.test, x.var=FRSTUSE1, rug=T, which.class="1", 
                        main = "FRSTUSE1 and Treatment Completion",
                        xlab= "Age at First Use", ylab= "Log Odds Completion", ylim = c(-0.1, 0.3), cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)

partialPlot(fit.rf, pred.data=drugdata.test, x.var=AGE, rug=T, which.class="1", 
                        main = "AGE and Treatment Completion",
                        xlab= "Age at Admission", ylab= "Log Odds Completion", ylim = c(-0.1, 0.3), cex.main=0.75,  cex.lab=0.75, cex.axis=0.75)
```


# 5. Conclusions, Implications & Limitations

## 5.1 Conclusions & Implications

TEDS-D captures a significant share of all discharges from treatment facilities across the United States, especially those that reflect public spending. As such, the findings from our model and our analysis are -- to an extent -- generalizable. Given that every dollar spent on substance use disorder treatment saves 4 dollars in health care costs and 7 dollars in criminal justice costs, our analysis has far-reaching public policy and financial implications. 

In considering the results discussed above, targeted intervention should manifest in a few ways. 

First, as we saw through analysis of `LOS` and log odds of treatment completion, just getting people in the door increases their chances of finishing the treatment stent. As such, increasing recruitment should be a top priority. Hiring social workers affiliated with the substance use treatment facilities who comb through the community or serve as supportive points of contact for struggling families would serve as a nice starting point. More importantly, seeing as the longer you spend in the program, the better your outcome, it is absolutely critical that policy targets major insurance companies to force funding of these treatment programs for longer than 2-4 weeks (as is current status).

We also noted a significant effect of geography on treatment completion. In light of diminishing success rates in the North East where the magnitude of the drug problem might be greater, finances and resources should be concentrated around areas like Boston, New York, and Philadelphia. At the same time, finding and infrastructure should be delegated to mountainous regions of the country where treatment completion also suffers -- perhaps because more isolated conditions inhibit treatment opportunities and treatment seeking. 

On top of geography, we noted that employment and economic resources/well-being are particularly predictive of treatment completion outcomes. Since it appears as though having a full-time position and a salary to return to sets one up for a complete recovery, it makes sense to establish funding initiatives or non-profits that work to establish short-term (or, ideally, longer-term) employment opportunities for patients who complete the program. In this sense, not only will the individual's time -- which might have contributed to drug seeking -- be occupied following discharge, but completing the program now has some sort of a financial incentive. One model that has seen tremendous success lately is Durham, North Carolina's Triangle Residential Options for Substance Abusers (TROSA). This organization contracts out its members to moving companies, thrift stores, etc. not only to subsidize the cost of their treatment, but also with the idea that -- following program completion -- they have a resume and a chance at future, stable employment opportunities. A model like this is certainly extendable to the North East. 

Additionally, since log odds were particularly low among individuals receiving disability/pensions, perhaps requiring a drug screening or a drug-based online course prior to receiving that money could be helpful in targeting that specific demographic.  

Another critical aspect of the economic side to substance use and abuse is self-medication. It's well-known that there is substantial comorbidity between SUDs and other psychiatric disorders among children and, in particular, adolescents. In our sample alone, having a psychiatric diagnosis was the 6th most predictive factor relative to treatment completion. If one's mental health situation is unstable and the economic situation is unfavorable, it might be easier and more affordable to borrow medication from a friend compared to making a doctors appointment and filling a prescription. 

* Prevalence of PSYCH symptoms in this population 
* Idea of exogenous DA fixing dysfunctions (like ADHD) that might be DA-based
* Homelessness -- untreated medical conditions

Last but not least, policy informed by our model and our analysis should continue to support and to target interventions through the avenues we know to be working: employers, the courts/criminal justice system, drug/alcohol abuse mentors, and -- in particular --schools. Since we know patients in their teens and early twenties to have the lowest log odds of treatment completion, but, at the same time, to be among the most amenable to positive treatment outcomes if admitted early, publicizing and de-stagmatizing these programs is a necessity.


## 5.2 Limitations
Although the study utilized a large, geographically diverse administrative dataset of annual discharges, it has several limitations. 

Facility-level characteristics, such as type of ownership (i.e. public or privatized), size of the facility, patient-to-provider ratio, service offerings, and other characteristics can play an important role in influencing treatment completion (Arndt et al., 2013). However, these facility-level variables were not available in the TEDS-D and, therefore, were not included in our analysis. 

Error associated with self-report cannot be accounted for in the dataset. There is no real incentive for patients to be transparent or forthcoming with respect to the drugs/substances playing a role in their admission, the frequency of their substance use, etc. In this sense, our analysis hinges on highly-personal information that, in many cases, cannot be verified. 

All data included in this analysis came from 2006. The accuracy and fluctuation of substance use and abstinence data would be more appropriately and more fully captured by long-term/longitudinal reports extending far beyond the treatment milieu. 

Critically, despite the flexible way in which our final model (`randomForest`) and associated `CART` algorithms can respond to data, substantial bias is still a real possibility. At the end of the day, the algorithms are trying in a single-minded manner to use associations in the data to maximize the homogeneity of data partitions. How those associations come to be represented have no foundation in subject matter understanding. While it is possible to peer into the black box, no matter how much information you can extract, `randomForest` remains a prediction tool and not a descriptive tool. Moreover, portions of our analysis rely on variable importance plots in which only one variable can be shuffled at a time. As a result, there is no consideration of joint importance over several predictors. This can be particularly detrimental when variables are correlated (i.e. there is a contribution to prediction accuracy that is uniquely linked to each predictor **and** joint contributions shared between two or more predictors).

Finally, in 2006 the main drugs of use/abuse were alcohol, cannabis, and cocaine. This is in stark contrast to where we stand today admits our nation's largest opioid epidemic. In recognizing that the number of patients entering substance use disorder treatment programs for opioid-related addiction, it is both possible and likely that predictors of treatment completion may be different among this population. As a result, it is difficult -- if not impossible-- to extend or predict the performance of our model relative to this change in substance use patterns.

Nonetheless, our final model enables data-driven discussions that have the potential to design interventions specific to improving treatment outcomes for patients struggling with SUD. It, combined with our overarching and well-supported analysis, paves the way for future research needed to inform specific interventions and policy designs that will be most effective in encouraging treatment completion for SUD. 
