---
title: "TEDS-D Data Exploration and Cleaning"
author: "Valerie Jill Sydnor"
date: "4/26/2019"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
    fig_caption: yes
    keep_tex: yes
  html_document:
    code_folding: show
    highlight: pygments 
    toc: yes
    toc_depth: 4
    toc_float: yes
    df_print: kable
theme: paper
geometry: margin=1.9cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, ISLR, rpart, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, rsample, gbm, SnowballC, ranger, BiocManager, keras, neuralnet, xtable, tidyverse, mapproj, psych, caret, logisticPCA, JOUSBoost)
knitr::opts_chunk$set(options(xtable.comment = FALSE))
knitr::opts_chunk$set(comment = " ")
```

____________________________________Data Cleaning and Variable Selection____________________________________

```{r eval=FALSE, include=FALSE}
# Read in Data:
drugdata <- read.csv("../TEDSD2006.csv", header=T, na.strings="-9")
```

```{r eval=FALSE, include=FALSE}
# Remove 24-Hour Detoxes and Ambulatory Detox:
drugdata.cleaned <- drugdata %>% filter(SERVSETD == "4" | SERVSETD == "5" | SERVSETD == "6" | SERVSETD == "7")
```


```{r,  eval=FALSE, include=FALSE}
# Remove Participants with Prior Treatment:
drugdata.cleaned <- drugdata.cleaned %>% filter(NOPRIOR == 0)
``` 


```{r,  eval=FALSE, include=FALSE}
# Remove Columns of Non-Interest, Low Interpretability or Redundancy:
drugdata.cleaned <- drugdata.cleaned %>% dplyr::select(-FREQ2, -FREQ3, -FRSTUSE2, -FRSTUSE3, -ROUTE2, -ROUTE3, -STFIPS, -SUB2, -SUB3, -CBSA, -PMSA, -DSMCRIT, -CASEID, -DISYR, -REGION, - NOPRIOR, -ARRESTS)
```


```{r,  eval=FALSE, include=FALSE}
# Look at NAs per Column:
sapply(drugdata.cleaned, function(x) sum(is.na(x)))
```


```{r,  eval=FALSE, include=FALSE}
# Remove Columns with too many NAs:
# DETNLF = 75% NAs
# PREG = 67%
# DETCRIM = 70%
# HLTHINS = 45%
# PRIMPAY = 52%
drugdata.cleaned <- drugdata.cleaned %>% dplyr::select(-DETNLF, -PREG, -DETCRIM, -PRIMPAY, -HLTHINS)
```


```{r,  eval=FALSE, include=FALSE}
# Remove Rows with NAs:
drugdata.cleaned <- na.omit(drugdata.cleaned)
sapply(drugdata.cleaned, function(x) sum(is.na(x))) #check
```


```{r,  eval=FALSE, include=FALSE}
# Look at Variable Class:
for(var in c("AGE", "GENDER","RACE","ETHNIC","MARSTAT", "EMPLOY", "EDUC","VET",
             "LIVARAG", "PRIMINC", "DIVISION", "SERVSETD", "METHUSE", 
             "PSOURCE", "SUB1", "ROUTE1", "FREQ1", "FRSTUSE1", "IDU", "ALCDRUG","PSYPROB")){
      drugdata.cleaned[,var] <- as.factor(drugdata.cleaned[,var])
}


drugdata.cleaned[ , grepl('FLG', names(drugdata.cleaned)) ] <- lapply(drugdata.cleaned[ , grepl('FLG' , names(drugdata.cleaned)) ], factor)    
sapply(drugdata.cleaned, function(x) class(x))
```


```{r,  eval=FALSE, include=FALSE}
# RECODE VARIABLE OF INTEREST:
for(num in c(2:8)){
drugdata.cleaned$REASON[drugdata.cleaned$REASON == num] <- 0
}
```

```{r,  eval=FALSE, include=FALSE}
# Look at Within-Variable Variability:
sapply(drugdata.cleaned, function(x) table(x))
```

```{r,  eval=FALSE, include=FALSE}
# Write Out Finalized Dataset:
write.csv(x = drugdata.cleaned, file="./Data/TEDS-D_DrugTreatment_Outcomes_Final.csv", sep=",", row.names = FALSE)
```

____________________________________Read in Cleaned Data____________________________________

```{r include=FALSE}
# Read in Cleaned Data
drug.data <- read.csv("./Data/TEDS-D_DrugTreatment_Outcomes_Final.csv", sep=",", header=T)

drug.data <- drug.data %>% rename(TREATCOMPLETE = REASON)

#Change Variable Classes
for(var in c("AGE", "GENDER","RACE","ETHNIC","MARSTAT", "EMPLOY", "EDUC","VET",
             "LIVARAG", "PRIMINC", "TREATCOMPLETE","DIVISION", "SERVSETD", "METHUSE", 
             "PSOURCE", "SUB1", "ROUTE1", "FREQ1", "FRSTUSE1", "IDU", "ALCDRUG","PSYPROB")){
      drug.data[,var] <- as.factor(drug.data[,var])
}

drug.data[ , grepl('FLG', names(drug.data)) ] <- lapply(drug.data[ , grepl('FLG' , names(drug.data)) ], factor)    
```

____________________________________Sample Info____________________________________

```{r sample characteristics, eval=FALSE, include=FALSE}
#Demographics 
table(drug.data$AGE)
table(drug.data$GENDER)
table(drug.data$RACE)
table(drug.data$MARSTAT)
table(drug.data$EDUC)
table(drug.data$EMPLOY)

#Drug Use Information
table(drug.data$SUB1)
table(drug.data$FRSTUSE1)
table(drug.data$ALCDRUG)

#Treatment Information
table(drug.data$SERVSETD)
table(drug.data$DAYWAIT)
table(drug.data$LOS)
```

____________________________________PCA on Admission Drugs____________________________________

```{r PCA Set Up, include=FALSE}
# Logistic Principal Component Analysis for Dimensionality Reduction of Drug Admissions Data
# Input: Drug Flags --> Flags indicate whether each drug was reported as in use at admission (0=no, 1=yes) for 18 drugs

# Step 1. Get drug flag data for input into logistic PCA
pca.data <- drug.data[, c(24:41)]
for(var in c(1:18)){
      pca.data[,var] <- as.numeric(pca.data[,var]) 
}
pca.data$ALCFLG[pca.data$ALCFLG == 1] <- 0
pca.data$ALCFLG[pca.data$ALCFLG == 2] <- 1
pca.data$COKEFLG[pca.data$COKEFLG == 1] <- 0
pca.data$COKEFLG[pca.data$COKEFLG == 2] <- 1
pca.data$MARFLG[pca.data$MARFLG== 1] <- 0
pca.data$MARFLG[pca.data$MARFLG == 2] <- 1
pca.data$HERFLG[pca.data$HERFLG== 1] <- 0
pca.data$HERFLG[pca.data$HERFLG == 2] <- 1
pca.data$METHFLG[pca.data$METHFLG== 1] <- 0
pca.data$METHFLG[pca.data$METHFLG == 2] <- 1
pca.data$OPSYNFLG[pca.data$OPSYNFLG== 1] <- 0
pca.data$OPSYNFLG[pca.data$OPSYNFLG == 2] <- 1
pca.data$PCPFLG[pca.data$PCPFLG== 1] <- 0
pca.data$PCPFLG[pca.data$PCPFLG == 2] <- 1
pca.data$HALLFLG[pca.data$HALLFLG == 1] <- 0
pca.data$HALLFLG[pca.data$HALLFLG == 2] <- 1
pca.data$MTHAMFLG[pca.data$MTHAMFLG== 1] <- 0
pca.data$MTHAMFLG[pca.data$MTHAMFLG == 2] <- 1
pca.data$AMPHFLG[pca.data$AMPHFLG== 1] <- 0
pca.data$AMPHFLG[pca.data$AMPHFLG == 2] <- 1
pca.data$STIMFLG[pca.data$STIMFLG== 1] <- 0
pca.data$STIMFLG[pca.data$STIMFLG == 2] <- 1
pca.data$BENZFLG[pca.data$BENZFLG== 1] <- 0
pca.data$BENZFLG[pca.data$BENZFLG == 2] <- 1
pca.data$TRNQFLG[pca.data$TRNQFLG== 1] <- 0
pca.data$TRNQFLG[pca.data$TRNQFLG == 2] <- 1
pca.data$BARBFLG[pca.data$BARBFLG== 1] <- 0
pca.data$BARBFLG[pca.data$BARBFLG == 2] <- 1
pca.data$SEDHPFLG[pca.data$SEDHPFLG== 1] <- 0
pca.data$SEDHPFLG[pca.data$SEDHPFLG == 2] <- 1
pca.data$INHFLG[pca.data$INHFLG== 1] <- 0
pca.data$INHFLG[pca.data$INHFLG == 2] <- 1
pca.data$OTCFLG[pca.data$OTCFLG== 1] <- 0
pca.data$OTCFLG[pca.data$OTCFLG == 2] <- 1
pca.data$OTHERFLG[pca.data$OTHERFLG== 1] <- 0
pca.data$OTHERFLG[pca.data$OTHERFLG == 2] <- 1
```

```{r PCA Tune, eval=FALSE, include=FALSE}

# Step 2. Tune Parameter K. Choose K=6!
logpca_model_k1_m5 = logisticPCA(pca.data, k = 1, m = 5)
logpca_model_k1_m5$prop_deviance_expl #27%

logpca_model_k2_m5 = logisticPCA(pca.data, k = 2, m = 5)
logpca_model_k2_m5$prop_deviance_expl #53%

logpca_model_k3_m5 = logisticPCA(pca.data, k = 3, m = 5)
logpca_model_k3_m5$prop_deviance_expl #70%

logpca_model_k4_m5 = logisticPCA(pca.data, k = 4, m = 5)
logpca_model_k4_m5$prop_deviance_expl #79%

logpca_model_k5_m5 = logisticPCA(pca.data, k = 5, m = 5)
logpca_model_k5_m5$prop_deviance_expl #84%

logpca_model_k6_m5 = logisticPCA(pca.data, k = 6, m = 5)
logpca_model_k6_m5$prop_deviance_expl #88%

logpca_model_k7_m5 = logisticPCA(pca.data, k = 7, m = 5)
logpca_model_k7_m5$prop_deviance_expl #90%

#Step 3. Tune Parameter M (if m not specified, best m is solved for at given K)
logpca_final_model = logisticPCA(pca.data, k = 6, m=0)
logpca_final_model$prop_deviance_expl #94%
save(logpca_final_model, file="./logpca_final_model.Rdata") #Save out results of PCA
```

```{r PCA Get PCs, include=FALSE}
load("./logpca_final_model.Rdata") #Read in results of PCA

# Step 4. Add PC Scores to drug data 
PCs <- data.frame(logpca_final_model$PCs) 
drug.data$PC1 <- PCs$X1
drug.data$PC2 <- PCs$X2
drug.data$PC3 <- PCs$X3
drug.data$PC4 <- PCs$X4
drug.data$PC5 <- PCs$X5
drug.data$PC6 <- PCs$X6
drug.data <- drug.data %>% select(-ends_with("FLG"))
```

__________Extract Training (N=40,000), Testing (N=20,000) and Validation (N=5,009) Datasets_____________

```{r include=FALSE}

#Extract training and testing data
set.seed(256)

N <- nrow(drug.data)
index.train <- sample(N, 45000)
drugdata.train <- drug.data[index.train,]
drugdata.test <- drug.data[-index.train,]

N <- nrow(drugdata.test)
index.validation <- sample(N, 5009)
drugdata.validation <- drugdata.test[index.validation,]
drugdata.test <- drugdata.test[-index.validation,]
```

____________________________________Statistics____________________________________

GLM with All Predictors (Val):

```{r GLM Full Model Val, include=FALSE}

# Step 1. Fit a logistic regression with all possible predictors
glm.allpredictors <- glm(TREATCOMPLETE ~ ., family=binomial, data=drugdata.train) 

# Step 2. Get fitted values and classification for testing data
glm.allpredictors.fittedprobabilities <- predict(glm.allpredictors, drugdata.test, type="response")  #Get probabilities
glm.allpredictors.classify <- ifelse(glm.allpredictors.fittedprobabilities > 0.5, "1", "0") #Classify

# Step 3. Evaluate

#MCE
testingerror.glm.allpredictors.MCE <- mean(drugdata.test$TREATCOMPLETE != glm.allpredictors.classify) # MCE= 0.3077 = FINAL TESTING ERROR FOR A LOGISTIC REGRESSION WITH ALL PREDICTORS

#AUC
glm.allpredictors.roc <- roc(drugdata.test$TREATCOMPLETE, glm.allpredictors.fittedprobabilities, plot=F) #AUC= 0.7561
```

Neural Net (Val):

```{r eval=FALSE, include=FALSE, Neural Net Val, echo=F}

# Step 1. Define the Model

model <- keras_model_sequential() %>%
  layer_dense(units = 20, activation = "relu", input_shape = c(30)) %>% 
  layer_dense(units = 10, activation = "relu") %>%
   layer_dense(units = 10, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

# Step 2. Compile the Model

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

#Step 3. Train the Model
## Determine Epoch number using training and validation data sets

### Format Data
x_train <- as.matrix(drugdata.train[,-15])
y_train <- as.matrix(drugdata.train[,15])
x_validate <- as.matrix(drugdata.validation[,-15])
y_validate <- as.matrix(drugdata.validation[,15])
x_test <- as.matrix(drugdata.test[,-15])
y_test <- as.matrix(drugdata.test[,15])

### Train
neuralnet.tune <- model %>% fit(x_train, 
                                y_train,
                                epochs = 30, 
                                batch_size = 1000, 
                                validation_data = list(x_validate, y_validate)
                                )

```

```{r include=FALSE}
# Step 4. Refit Model with Chosen Parameters

set.seed(1)
# FINAL MODEL
model <- keras_model_sequential() %>%
  layer_dense(units = 20, activation = "relu", input_shape = c(30)) %>% 
  layer_dense(units = 10, activation = "relu") %>%
   layer_dense(units = 10, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

x_train <- as.matrix(drugdata.train[,-15])
y_train <- as.matrix(drugdata.train[,15])
x_test <- as.matrix(drugdata.test[,-15])
y_test <- as.matrix(drugdata.test[,15])

NN.allpredictors <- model %>% fit(x_train, y_train, epochs = 25, batch_size = 1000)

# Step 5. Evaluate 

## AUC 
results <- model %>% evaluate(x_test, y_test)

## MCE
MCE.NN <- 1-results$acc # MCE = 0.332 = FINAL TESTING ERROR FOR NEURAL NET

NN_predict_classes <- predict_classes(object = model,
                                    x = as.matrix(x_test)) %>%
  as.vector()

testingerror.NN.MCE <- mean(y_test != NN_predict_classes) 


#AUC Plot
NN_predictions <- predict(object = model,
                                    x = as.matrix(x_test)) %>%
  as.vector()

NN.roc <- roc(drugdata.test$TREATCOMPLETE, NN_predictions, plot=F) #AUC = 0.7447
```

Boosting Trees ADABOOST (Val):

```{r Boosting Val, eval=FALSE, include=FALSE}

# Step 1. Format Data
x.train <- (drugdata.train[,-(15)]) 
y.train <- as.numeric(drugdata.train[,15]) -1
y.train[y.train == 0] = -1

x.test <- (drugdata.test[,-(15)])
y.test <- as.numeric(drugdata.test[,15]) -1
predictions

# Step 2. Boost
## Tune Parameters
###Tune tree depth (try 3, 5, 6, 7, 8, 10). We choose a tree depth of 6 to lower testing error!
boost.3.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 3, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.3.30, data.matrix(x.train))
train_err.3.30 <- mean(y.train != yhat_train_ada) #0.2944
yhat_test_ada <- predict(boost.3.30, data.matrix(x.test))
test_err.3.30 <- mean(y.test != yhat_test_ada) #0.301

boost.5.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 5, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.5.30, data.matrix(x.train))
train_err.5.30 <- mean(y.train != yhat_train_ada) #0.275
yhat_test_ada <- predict(boost.5.30, data.matrix(x.test))
test_err.5.30 <- mean(y.test != yhat_test_ada) #0.2907


boost.6.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 6, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.6.30, data.matrix(x.train))
train_err.6.30 <- mean(y.train != yhat_train_ada) #0.2578
yhat_test_ada <- predict(boost.6.30, data.matrix(x.test))
test_err.6.30 <- mean(y.test != yhat_test_ada) #0.28425

boost.7.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 7, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.7.30, data.matrix(x.train))
train_err.7.30 <- mean(y.train != yhat_train_ada) #0.242
yhat_test_ada <- predict(boost.7.30, data.matrix(x.test))
test_err.7.30 <- mean(y.test != yhat_test_ada) #0.2863

boost.8.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 8, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.8.30, data.matrix(x.train))
train_err.8.30 <- mean(y.train != yhat_train_ada) #0.2165
yhat_test_ada <- predict(boost.8.30, data.matrix(x.test))
test_err.8.30 <- mean(y.test != yhat_test_ada) #0.292

boost.10.30 <- adaboost(data.matrix(x.train), y.train, tree_depth = 10, n_rounds = 30,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.10.30, data.matrix(x.train))
train_err.10.30 <- mean(y.train != yhat_train_ada) #0.1148
yhat_test_ada <- predict(boost.10.30, data.matrix(x.test))
test_err.10.30 <- mean(y.test != yhat_test_ada) #0.2979

#Tune n_rounds

boost.6.50 <- adaboost(data.matrix(x.train), y.train, tree_depth = 6, n_rounds = 50,
                    verbose = FALSE,
                   control = NULL)
yhat_train_ada <- predict(boost.6.50, data.matrix(x.train))
train_err.6.50 <- mean(y.train != yhat_train_ada) #0.247
yhat_test_ada <- predict(boost.6.50, data.matrix(x.test))
test_err.6.50 <- mean(y.test != yhat_test_ada) #0.28422
```

```{r FINAL BOOSTING TREES, include=FALSE}

#FINAL BOOSTING TREES MODEL
x.train <- (drugdata.train[,-(15)]) 
y.train <- as.numeric(drugdata.train[,15]) -1
y.train[y.train == 0] = -1

x.test <- (drugdata.test[,-(15)])
y.test <- as.numeric(drugdata.test[,15]) -1
y.test[y.test == 0] = -1

#boost.6.100 <- adaboost(data.matrix(x.train), y.train, tree_depth = 6, n_rounds = 500,
  #                  verbose = FALSE,
   #                control = NULL)
#save(boost.6.100, file="./final_boosting_model.Rdata") #Save out final model
load("./final_boosting_model.Rdata")

# Step 3. Evaluate

#yhat_train_ada <- predict(boost.6.100, data.matrix(x.train))
#train_err.6.100 <- mean(y.train != yhat_train_ada) #0.2231 = TRAINING ERROR 0.2231

#MCE
yhat_test_ada <- predict(boost.6.100, data.matrix(x.test))
testingerror.boosting.6.100.MCE <- mean(y.test != yhat_test_ada) # MCE = 0.27945 = FINAL TESTING ERROR FOR BOOSTING TREES

#AUC
boosting.fitted <- predict(boost.6.100, data.matrix(drugdata.test), type="response") 
boosting.fitted[boosting.fitted == -1] = 0
boosting.roc <- roc(drugdata.test$TREATCOMPLETE, boosting.fitted, plot=F) #AUC = 0.72
```

Boosting Trees GBM (Val):

```{r Boosting Val GBM, eval=FALSE, include=FALSE}

# Step 1. Format Data
x.boost.train <- drugdata.train[,-15]
y.boost.train <- make.names(drugdata.train[,15])
x.boost.test <- drugdata.test[,-15]
y.boost.test <- drugdata.test[,15]

# Step 2: Tune Boosting Parameters
fitControl <- trainControl(method = "cv",
                           number = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)
gbmGrid <-  expand.grid(interaction.depth = c(2, 4, 6, 8), 
                        n.trees = (1:10)*100, 
                        shrinkage = 0.01,
                        n.minobsinnode = 20)
GBM.boosting.model <- train(x.boost.train,y.boost.train, 
                 method='gbm', 
                  trControl=fitControl,
                  tuneGrid = gbmGrid,
                  metric = "ROC")
print(GBM.boosting.model)
save(GBM.boosting.model, file="./GBM_Boosting_Model.Rdata") #Save out final model
```

```{r include=FALSE}
x.boost.train <- drugdata.train[,-15]
y.boost.train <- make.names(drugdata.train[,15])
x.boost.test <- drugdata.test[,-15]
y.boost.test <- drugdata.test[,15]

load("./GBM_Boosting_Model.Rdata")

# Step 3. Evaluate
predictions <- predict(object=GBM.boosting.model, x.boost.test, type='prob')
boosting.probabilities <- predictions[,2]
boosting.classify <- ifelse(boosting.probabilities > 0.5, "1", "0")
testingerror.boost.MCE <- mean(drugdata.test$TREATCOMPLETE != boosting.classify) #TESTING ERROR = 0.267
gbmboost.roc <- roc(drugdata.test$TREATCOMPLETE, boosting.probabilities, plot=F) #AUC = 0.8052
importantpredictors <- summary(GBM.boosting.model)
```

```{r echo=FALSE}
# Boosting 15 MOST IMPORTANT PREDICTORS
importantpredictors.df <- data.frame(importantpredictors)
rownames(importantpredictors.df) <- c()
importantpredictors.df <- importantpredictors.df %>% rename(Variable = var)
importantpredictors.df <- importantpredictors.df %>% rename(Relative_Influence = rel.inf)
importantpredictors.df[1:15,]
```

```{r echo=FALSE}
# BOOSTING PREDICTOR RELATIVE INFLUENCE GRAPH
plot(varImp(GBM.boosting.model,scale=F, xlab="Variable Importance"))
```

Compare Val Models (GLM, NN, Boosting) with AUC CURVES:

```{r PLOT ROCs,fig.height=5, fig.width=9, echo=F}
par(mfrow=c(1,1))
plot(1-NN.roc$specificities, NN.roc$sensitivities, col="steelblue3", pch=16, cex=0.4, xlab= "False Positives", ylab="Sensitivities", font.lab=2, main="Model Comparisons: ROC Curves", font.main=2,
text(.75, .2, paste("\nModel 1: Boosting AUC=", round(pROC::auc(gbmboost.roc),3),
                "\nModel 2: GLM AUC=",round(pROC::auc(glm.allpredictors.roc),3), 
                "\nModel 3: Neural Net AUC=",round(pROC::auc(NN.roc),3))))

points(1-glm.allpredictors.roc$specificities, glm.allpredictors.roc$sensitivities, col="mediumpurple3", pch=16, cex=0.4)


points(1-gbmboost.roc$specificities, gbmboost.roc$sensitivities, col="springgreen4", pch=16, cex=0.4)


legend(0, 1, legend=c("Model 1", "Model 2", "Model 3"),
       col=c("springgreen4", "mediumpurple3", "steelblue3"), lty=1:1, cex=0.8)

lines(1-gbmboost.roc$specificities, gbmboost.roc$sensitivities, lwd=2, col="springgreen4")
lines(1-glm.allpredictors.roc$specificities, glm.allpredictors.roc$sensitivities, lwd=2, col="mediumpurple3")
lines(1-NN.roc$specificities, NN.roc$sensitivities, lwd=2, col="steelblue3")

```

# 1. Introduction 

## 1.1. Scope of the Problem
## 1.2. Project Goals

# 2. Methods
## 2.1. The Treatment Episode Data Set—Discharges (TEDS-D) Data

### Data Overview
The Treatment Episode Data Set—Discharges (TEDS-D) is a public dataset made available by the US Department of Health and Human Services that contains patient-level information for individuals treated for substance use disorders in the United States during the year 2006.  The TEDS-D data includes variables related to patient demographics (e.g., age, sex, race), geographics (e.g. census division), and psychographics (e.g. employment); patient drug use (e.g. primary substance abused, age of first use, frequency of use); SUD treatment program (e.g. rehabilitation center type, length of treatment stay); and SUD risk factors (e.g. comorbid psychiatric diagnoses, veteran status, living situation). 

The binary outcome variable of interest included in the TEDS-D dataset and used as the dependent variable in all statistical models developed here is **Treatment Completed** (yes=1 or no=0). A **completed treatment** indicates that all parts of the treatment plan were completed prior to patient discharge from the treatment program. On the other hand, **treatment non-completion** denotes that the patient left or was discharged prior to treatment completion; the most frequent causes of treatment non-completion included a patient leaving the program early against professional advice, program termination by the treatment facility due to patient non-compliance, or patient transfer to another facility. 

### Data Cleaning: Selection of Predictor Variables and Discharge Entries
The original TEDS-D dataset included 65 variables and 1,048,575 observations. Critically, however, each observation included in this original dataset represented *one hospital discharge*, rather than *one unique patient*. In order to avoid introducing bias into our predictive models via the presence of multiple entries for the same patient, we removed all observations for which the number of prior treatment episodes in a drug or alcohol program was greater than 0. This ensured that each discharge entry and all relevant variables were indeed obtained from a unique patient. We additionally removed all entries wherein the patient was discharged from a 24 hour detoxification service, as detoxification and treatment programs are quite distinctive with regards to program settings and goals. Consequently, we focused our analyses on short (< 30 days) and long term (> 30 days) treatment programs that were inpatient, partial hospitalization, or intensive outpatient programs. The application of these exclusion criteria (prior treatment program participation and 24 hour detoxification service) to the original dataset resulted in a subsample of 265,111 eligible observations. 

Prior to developing statistical models aimed at predicting treatment completion versus non-completion, we additionally removed predictor variables that were unique to each patient, redundant, or non-variable across entries. Examples of redundant variables excluded from the dataset include state code, metropolitan area, and census region (redundant with regards to census division) and secondary and tertiary route of drug administration. Predictors that were non-variable across discharge entries and removed from the dataset included year of discharge and number of prior SUD treatment programs participated in. Next, we looked at the number of missing values per each predictor variable and removed those with a large number of missing values (>40 % of entries missing data). The following variables were removed for missing data: number of arrests in 30 days prior to admission, DSM diagnosis, detailed not in labor force status, pregnant at time of admission (yes/no), detailed criminal justice referral, health insurance type, and primary source of treatment program payment. After finalizing the set of predictor variables, we removed any discharge entries that still had missing values for one or more of the predictors. The final cleaned dataset included 42 predictor variables and 70,009 observations.


### Treatment Success Predictors
The following variables were considered potential predictors of treatment completion versus non-completion for all statistical models: age, sex, race, ethnicity, marital status, years of education, employment status, primary source of income, living arrangement, veteran status, census division, service setting at discharge, number of days patient waited before entering treatment program, length of treatment stay, principal source of treatment program referral, primary SUD, primary route of drug administration, frequency of drug use, age of first drug use, number of substances reported at admission, IV drug use reported at admission, prescribed a pharmacological opioid therapy at admission, alcohol/drug substance use type (alcohol only, drugs only, both), and comorbid psychiatric diagnosis (yes/no). The cleaned data additionally included 18 variables pertaining to whether 18 different substances were reported as a substance of use/abuse at admission (i.e. “drug flags” at admission). In order to reduce the number of statistical predictors, increase model degrees of freedom, and better capture variance across individuals in the sample, a logistic principal components analysis (PCA) was performed on the 18 drug flags. This data reduction strategy allowed for the identification of a small number of principal components that maximally captured variance in drug use at admission. Logistic PCA solutions with 1-7 principal components were tested, and a 6 principal component solution was chosen; the 6 principal components identified explained 94% of drug flag variable variance. Principal component scores for the 6 components were generated for each observation, and these 6 scores were used as potential model predictors in addition to the 24 variables described above. The R function logisticPCA from package logisticPCA was used for this analysis.

## 2.2. Study Sample Characteristics

70,009 unique patients (46,986 male and 23,023 female patients) who were discharged from a substance use disorder treatment program in 2006 were included in this report. Patient ages ranged from 12-55+, and most patients were African American or Caucasian. Comprehensive sample demographics can be found in **Table 1**. 

With regards to substance use in the current sample, 42% of the sample had a primary alcohol use disorder, 30% were primarily cannabis users, and 15% of the sample reported a primary cocaine use disorder. About 3% of the sample reported primary use of heroin, and another 3% reported primary use of opioids. Of note, 52% of individuals reported regularly using more than 1 substance at treatment admission. The majority of the study sample reported that their age of first drug use of the drug listed as “primary” was 17 or younger (7% at age 11 or under, 23% at ages 12-14, and 33% at ages 15-17).  Comorbid drug and alcohol use disorders were quite prevalent, with 40% of the sample reporting combined alcohol and drug SUDs (35% reported only a drug SUD, and 25% only an alcohol SUD). Interestingly, drug and alcohol disorder comorbidity was higher in the group of individuals that did not complete treatment (43% of this subsample had comorbid drug and alcohol SUDs) as compared to those that completed treatment (35% of this subsample had comorbid SUDs), as was the number of substances reported at admission (59% of treatment non-completers were using 2+ drugs, whereas only 45% of treatment completers were). Overall, substance use in this population of individuals seeking treatment appears to be chronic, complex, and highly comorbid, three factors that increase risk of treatment termination or relapse and that make predicting treatment completion critical.  

### Characteristics of Treatment Completed Versus Treatment Non-Completed Samples
KARA

## 2.3. Statistics

### Training, Testing, and Validation Data Split

In order to test the prediction accuracy of each of the statistical models developed in an unbiased manner (i.e. on data that was *not* used to train the models), the 70,009 observations included in the cleaned TEDS-D dataset were split into a training data cohort (N= 45,000), a validation data cohort (N= 5,009) and a testing data cohort (N= 20,000). Each model was trained using the same training data observations, and model classification accuracy was tested on the left out testing data. The smaller validation cohort was used only to tune neural network parameters.


### Statistical Models
#### Logistic Regression 

X logistic regression models were developed for the prediction of treatment completion versus non-completion. Training data was used in the estimation of beta coefficients, and the probability of treatment completion was calculated for each patient (and each model) using the formula: 

$$ P(Y=1 | X) = \frac{e^{\beta_0+\beta_1*X1 ... \beta_N*XN}}{1+e^{\beta_0+\beta_1*X1 ... +\beta_N*XN}}$$

Individual probabilities were used to classify patients into completers (1) and non-completers (0) using a binary cut-off threshold probability=0.5. The R function glm was used to run logistic regressions. 
	
**Logistic Model 1: All Predictors**: The first logistic model was fit by using all 30 predictors to predict treatment completion.  


#### Random Forest
KARA

#### Boosting Trees

Boosting is an extension of random forest, however rather than building a forest of *independent* trees using a set number of randomly chosen predictors per tree, each tree built during the boosting process attempts to *improve the prediction performance of the previously generated tree* (i.e. correct previous tree errors), thus boosted trees are *non-independent*. Boosting models improve probability and classification predictions by using an ensemble of decision trees that were generated (and iteratively improved upon) by modifying the weights of individual observations in the training data. More specifically, with boosting, trees fitted initially from equally or randomly weighted observations are improved upon for observations that were misclassified by increasing the weight of the misclassified observations. Over many iterations, weak learning trees with small depth can thus be converted into strong learners by changing input observation weights and optimizing a specified metric (here, accuracy/AUC). Of note, whereas in random forest all generated trees are weighted equally when generating final predictions, trees built during boosting are given different weights in the final predictive model.

The functions trainControl, train, and gbm from packages caret and gbm were used to develop an optimized boosting tree model. Boosting parameters were chosen by testing interaction depths of 2, 4, 6, and 8 and tree numbers from 500 to 1,000 (in increments of 100 trees), using a set shrinkage of 0.01 and a minimum of 20 observations per node. Parameters were tested using training data and cross-validation with five folds. The finalized parameters were: an interaction depth of 8, 1,000 trees, a shrinkage of 0.01 and a minimum of 20 observations per node. The code used for boosting is shown below:

fitControl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)
gbmGrid <-  expand.grid(interaction.depth = c(2, 4, 6, 8), n.trees = (1:10)*100, shrinkage = 0.01, n.minobsinnode = 20)
GBM.boosting.model <- train(x.boost.train,y.boost.train, method='gbm', trControl=fitControl,tuneGrid = gbmGrid,metric = "ROC")

#### Neural Networks 

JARED

Keras, neuralnet and tensor flow were used for deep learning.

### Model Comparisons: Misclassification Error and AUC


# 3. Results

## 3.1 Models
### Logistic Regression 
#### Least Absolute Shrinkage and Selection Operator
#### Elastic Net
### Random Forest
### Boosting Trees
### Neural Nets

## 3.2. Model Comparisons

# 4. Conclusions and Implications

